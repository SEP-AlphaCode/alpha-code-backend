<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/Dockerfile">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Dockerfile" />
              <option name="originalContent" value="FROM python:3.10-slim&#10;&#10;WORKDIR /app&#10;&#10;RUN apt-get update &amp;&amp; apt-get install -y \&#10;    build-essential \&#10;    ffmpeg \&#10;    libsndfile1 \&#10;    &amp;&amp; rm -rf /var/lib/apt/lists/*&#10;&#10;COPY requirements.txt .&#10;RUN pip install --no-cache-dir --upgrade pip \&#10;    &amp;&amp; pip install --no-cache-dir -r requirements.txt&#10;&#10;COPY . .&#10;&#10;ENV PYTHONPATH=&quot;/app/midas_repo:/app&quot;&#10;&#10;EXPOSE 8082&#10;&#10;CMD [&quot;uvicorn&quot;, &quot;main:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;8082&quot;]" />
              <option name="updatedContent" value="FROM python:3.10-slim&#13;&#10;&#13;&#10;WORKDIR /app&#13;&#10;&#13;&#10;RUN apt-get update &amp;&amp; apt-get install -y \&#13;&#10;    build-essential \&#13;&#10;    ffmpeg \&#13;&#10;    libsndfile1 \&#13;&#10;    &amp;&amp; rm -rf /var/lib/apt/lists/*&#13;&#10;&#13;&#10;COPY requirements.txt .&#13;&#10;RUN pip install --no-cache-dir --upgrade pip \&#13;&#10;    &amp;&amp; pip install --no-cache-dir -r requirements.txt&#13;&#10;&#13;&#10;COPY . .&#13;&#10;&#13;&#10;ENV PYTHONPATH=&quot;/app&quot;&#13;&#10;&#13;&#10;EXPOSE 8082&#13;&#10;&#13;&#10;CMD [&quot;uvicorn&quot;, &quot;main:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;8082&quot;]" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/app/routers/object_detect.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app/routers/object_detect.py" />
              <option name="originalContent" value="# app/routers/object_router.py&#10;from typing import List&#10;&#10;import torch&#10;from PIL import Image&#10;from fastapi import APIRouter, UploadFile, File, HTTPException&#10;from ultralytics import YOLO&#10;import cv2&#10;import numpy as np&#10;from models.midas.dpt_depth import DPTDepthModel&#10;import torchvision.transforms as transforms&#10;from app.models.object_detect import DetectClosestResponse, Detection&#10;&#10;router = APIRouter()&#10;&#10;# Load YOLO once (lazy load at module import)&#10;yolo_model = YOLO(&quot;models/yolo/yolov8l.pt&quot;)&#10;midas = DPTDepthModel(&#10;    path=&quot;models/midas/dpt_hybrid_384.pt&quot;,&#10;    backbone=&quot;vitb_rn50_384&quot;,&#10;    non_negative=True,&#10;)&#10;midas_transform = transforms.Compose([&#10;    transforms.Resize(384),&#10;    transforms.CenterCrop(384),&#10;    transforms.ToTensor(),&#10;    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),&#10;])&#10;device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)&#10;midas.to(device)&#10;midas.eval()&#10;&#10;&#10;def estimate_depth(image: np.ndarray) -&gt; np.ndarray:&#10;    &quot;&quot;&quot;Run MiDaS depth estimation and return normalized depth map.&quot;&quot;&quot;&#10;&#10;    # Convert OpenCV BGR -&gt; RGB and then to PIL&#10;    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)&#10;    pil_image = Image.fromarray(img_rgb)&#10;&#10;    # Apply MiDaS transform (returns a tensor, shape [3, H, W])&#10;    input_tensor = midas_transform(pil_image)&#10;&#10;    # Add batch dimension: [1, 3, H, W]&#10;    input_batch = input_tensor.unsqueeze(0).to(device)&#10;&#10;    with torch.no_grad():&#10;        prediction = midas(input_batch)&#10;        prediction = torch.nn.functional.interpolate(&#10;            prediction.unsqueeze(1),&#10;            size=image.shape[:2],&#10;            mode=&quot;bicubic&quot;,&#10;            align_corners=False,&#10;        ).squeeze()&#10;&#10;    depth_map = prediction.cpu().numpy()&#10;    # Normalize for easier comparison&#10;    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())&#10;&#10;    return depth_map&#10;&#10;@router.post(&quot;/detect_closest&quot;)&#10;async def detect_closest_objects(file: UploadFile = File(...), k: int = 3) -&gt; DetectClosestResponse:&#10;    try:&#10;        &quot;&quot;&quot;&#10;            Upload an image, detect objects with YOLO, estimate depth with MiDaS,&#10;            and return the k closest objects.&#10;            &quot;&quot;&quot;&#10;        # Read image into numpy array&#10;        &#10;        image_bytes = await file.read()&#10;        &#10;        nparr = np.frombuffer(image_bytes, np.uint8)&#10;        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)&#10;        &#10;        &#10;        # Step 1: Run YOLO&#10;        results = yolo_model(img)&#10;        &#10;        &#10;        # Step 2: Run depth estimation&#10;        depth_map = estimate_depth(img)&#10;        &#10;        &#10;        # Step 3: Collect detections with depth metrics&#10;        detections: List[Detection] = []&#10;        for r in results:&#10;            for box in r.boxes:&#10;                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())&#10;                label = r.names[int(box.cls)]&#10;                conf = float(box.conf)&#10;                &#10;                # Clip bounding box to image size&#10;                h, w = depth_map.shape&#10;                x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w - 1, x2), min(h - 1, y2)&#10;                &#10;                # Extract depth inside bounding box&#10;                roi = depth_map[y1:y2, x1:x2]&#10;                if roi.size == 0:&#10;                    continue&#10;                &#10;                # Compute closeness metrics&#10;                avg_depth = float(np.mean(roi))&#10;                min_depth = float(np.min(roi))  # closest pixel&#10;                median_depth = float(np.median(roi))&#10;                &#10;                detections.append(Detection(&#10;                        label=label,&#10;                        confidence=conf,&#10;                        bbox=[x1, y1, x2, y2],&#10;                        depth_avg=avg_depth,&#10;                        depth_min=min_depth,&#10;                        depth_median=median_depth,&#10;                    ))&#10;        filtered = [d for d in detections if d.label.lower() != &quot;person&quot;]&#10;        # Step 4: Sort by &quot;closeness&quot; (lowest depth = closest)&#10;        detections_sorted = sorted(filtered, key=lambda d: d.depth_min or 9999.0)&#10;        return DetectClosestResponse(closest_objects=detections_sorted[:k], all_objects=detections_sorted)&#10;    except Exception as e:&#10;        raise HTTPException(status_code=500, detail=str(e))" />
              <option name="updatedContent" value="# app/routers/object_router.py&#10;from typing import List&#10;import sys&#10;import os&#10;&#10;# Add the models directory to Python path for import resolution&#10;sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'models'))&#10;&#10;import torch&#10;from PIL import Image&#10;from fastapi import APIRouter, UploadFile, File, HTTPException&#10;from ultralytics import YOLO&#10;import cv2&#10;import numpy as np&#10;from midas.dpt_depth import DPTDepthModel&#10;import torchvision.transforms as transforms&#10;from app.models.object_detect import DetectClosestResponse, Detection&#10;&#10;router = APIRouter()&#10;&#10;# Load YOLO once (lazy load at module import)&#10;yolo_model = YOLO(&quot;models/yolo/yolov8l.pt&quot;)&#10;midas = DPTDepthModel(&#10;    path=&quot;models/midas/dpt_hybrid_384.pt&quot;,&#10;    backbone=&quot;vitb_rn50_384&quot;,&#10;    non_negative=True,&#10;)&#10;midas_transform = transforms.Compose([&#10;    transforms.Resize(384),&#10;    transforms.CenterCrop(384),&#10;    transforms.ToTensor(),&#10;    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),&#10;])&#10;device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)&#10;midas.to(device)&#10;midas.eval()&#10;&#10;&#10;def estimate_depth(image: np.ndarray) -&gt; np.ndarray:&#10;    &quot;&quot;&quot;Run MiDaS depth estimation and return normalized depth map.&quot;&quot;&quot;&#10;&#10;    # Convert OpenCV BGR -&gt; RGB and then to PIL&#10;    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)&#10;    pil_image = Image.fromarray(img_rgb)&#10;&#10;    # Apply MiDaS transform (returns a tensor, shape [3, H, W])&#10;    input_tensor = midas_transform(pil_image)&#10;&#10;    # Add batch dimension: [1, 3, H, W]&#10;    input_batch = input_tensor.unsqueeze(0).to(device)&#10;&#10;    with torch.no_grad():&#10;        prediction = midas(input_batch)&#10;        prediction = torch.nn.functional.interpolate(&#10;            prediction.unsqueeze(1),&#10;            size=image.shape[:2],&#10;            mode=&quot;bicubic&quot;,&#10;            align_corners=False,&#10;        ).squeeze()&#10;&#10;    depth_map = prediction.cpu().numpy()&#10;    # Normalize for easier comparison&#10;    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())&#10;&#10;    return depth_map&#10;&#10;@router.post(&quot;/detect_closest&quot;)&#10;async def detect_closest_objects(file: UploadFile = File(...), k: int = 3) -&gt; DetectClosestResponse:&#10;    try:&#10;        &quot;&quot;&quot;&#10;            Upload an image, detect objects with YOLO, estimate depth with MiDaS,&#10;            and return the k closest objects.&#10;            &quot;&quot;&quot;&#10;        # Read image into numpy array&#10;        &#10;        image_bytes = await file.read()&#10;        &#10;        nparr = np.frombuffer(image_bytes, np.uint8)&#10;        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)&#10;        &#10;        &#10;        # Step 1: Run YOLO&#10;        results = yolo_model(img)&#10;        &#10;        &#10;        # Step 2: Run depth estimation&#10;        depth_map = estimate_depth(img)&#10;        &#10;        &#10;        # Step 3: Collect detections with depth metrics&#10;        detections: List[Detection] = []&#10;        for r in results:&#10;            for box in r.boxes:&#10;                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())&#10;                label = r.names[int(box.cls)]&#10;                conf = float(box.conf)&#10;                &#10;                # Clip bounding box to image size&#10;                h, w = depth_map.shape&#10;                x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w - 1, x2), min(h - 1, y2)&#10;                &#10;                # Extract depth inside bounding box&#10;                roi = depth_map[y1:y2, x1:x2]&#10;                if roi.size == 0:&#10;                    continue&#10;                &#10;                # Compute closeness metrics&#10;                avg_depth = float(np.mean(roi))&#10;                min_depth = float(np.min(roi))  # closest pixel&#10;                median_depth = float(np.median(roi))&#10;                &#10;                detections.append(Detection(&#10;                        label=label,&#10;                        confidence=conf,&#10;                        bbox=[x1, y1, x2, y2],&#10;                        depth_avg=avg_depth,&#10;                        depth_min=min_depth,&#10;                        depth_median=median_depth,&#10;                    ))&#10;        filtered = [d for d in detections if d.label.lower() != &quot;person&quot;]&#10;        # Step 4: Sort by &quot;closeness&quot; (lowest depth = closest)&#10;        detections_sorted = sorted(filtered, key=lambda d: d.depth_min or 9999.0)&#10;        return DetectClosestResponse(closest_objects=detections_sorted[:k], all_objects=detections_sorted)&#10;    except Exception as e:&#10;        raise HTTPException(status_code=500, detail=str(e))" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>