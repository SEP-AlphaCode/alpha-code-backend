<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/Dockerfile">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Dockerfile" />
              <option name="originalContent" value="FROM python:3.10-slim&#10;&#10;WORKDIR /app&#10;&#10;RUN apt-get update &amp;&amp; apt-get install -y \&#10;    build-essential \&#10;    ffmpeg \&#10;    libsndfile1 \&#10;    &amp;&amp; rm -rf /var/lib/apt/lists/*&#10;&#10;COPY requirements.txt .&#10;RUN pip install --no-cache-dir --upgrade pip \&#10;    &amp;&amp; pip install --no-cache-dir -r requirements.txt&#10;&#10;COPY . .&#10;&#10;ENV PYTHONPATH=&quot;/app/midas_repo:/app&quot;&#10;&#10;EXPOSE 8082&#10;&#10;CMD [&quot;uvicorn&quot;, &quot;main:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;8082&quot;]" />
              <option name="updatedContent" value="FROM python:3.10-slim&#13;&#10;&#13;&#10;WORKDIR /app&#13;&#10;&#13;&#10;RUN apt-get update &amp;&amp; apt-get install -y \&#13;&#10;    build-essential \&#13;&#10;    ffmpeg \&#13;&#10;    libsndfile1 \&#13;&#10;    &amp;&amp; rm -rf /var/lib/apt/lists/*&#13;&#10;&#13;&#10;COPY requirements.txt .&#13;&#10;RUN pip install --no-cache-dir --upgrade pip \&#13;&#10;    &amp;&amp; pip install --no-cache-dir -r requirements.txt&#13;&#10;&#13;&#10;COPY . .&#13;&#10;&#13;&#10;ENV PYTHONPATH=&quot;/app&quot;&#13;&#10;&#13;&#10;EXPOSE 8082&#13;&#10;&#13;&#10;CMD [&quot;uvicorn&quot;, &quot;main:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;8082&quot;]" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/app/routers/object_detect.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app/routers/object_detect.py" />
              <option name="originalContent" value="# app/routers/object_router.py&#10;from typing import List&#10;import sys&#10;import os&#10;&#10;# Handle different directory structures between local and deployed environments&#10;current_dir = os.path.dirname(os.path.abspath(__file__))&#10;&#10;# Check if we're in a nested app/app structure (deployed) or app structure (local)&#10;if current_dir.endswith('/app/app/routers') or current_dir.endswith('\\app\\app\\routers'):&#10;    # Deployed environment: /app/app/routers -&gt; /app/models&#10;    project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))&#10;    models_dir = os.path.join(project_root, 'models')&#10;else:&#10;    # Local environment: project_root/app/routers -&gt; project_root/models&#10;    project_root = os.path.dirname(os.path.dirname(current_dir))&#10;    models_dir = os.path.join(project_root, 'models')&#10;&#10;# Add paths for import resolution&#10;sys.path.insert(0, models_dir)&#10;sys.path.insert(0, project_root)&#10;&#10;import torch&#10;from PIL import Image&#10;from fastapi import APIRouter, UploadFile, File, HTTPException&#10;from ultralytics import YOLO&#10;import cv2&#10;import numpy as np&#10;&#10;# Try multiple import approaches for maximum compatibility&#10;try:&#10;    print(&quot;DEBUG: Trying first import approach...&quot;)&#10;    from midas.dpt_depth import DPTDepthModel&#10;    print(&quot;DEBUG: First import successful&quot;)&#10;except ImportError as e1:&#10;    print(f&quot;DEBUG: First import failed: {e1}&quot;)&#10;    try:&#10;        print(&quot;DEBUG: Trying second import approach...&quot;)&#10;        from models.midas.dpt_depth import DPTDepthModel&#10;        print(&quot;DEBUG: Second import successful&quot;)&#10;    except ImportError as e2:&#10;        print(f&quot;DEBUG: Second import failed: {e2}&quot;)&#10;        try:&#10;            print(&quot;DEBUG: Trying third import approach (absolute path)...&quot;)&#10;            # Absolute path import as last resort&#10;            import importlib.util&#10;            midas_path = os.path.join(models_dir, 'midas', 'dpt_depth.py')&#10;            print(f&quot;DEBUG: Trying to load from: {midas_path}&quot;)&#10;            print(f&quot;DEBUG: File exists: {os.path.exists(midas_path)}&quot;)&#10;&#10;            if os.path.exists(midas_path):&#10;                spec = importlib.util.spec_from_file_location(&quot;dpt_depth&quot;, midas_path)&#10;                dpt_module = importlib.util.module_from_spec(spec)&#10;                spec.loader.exec_module(dpt_module)&#10;                DPTDepthModel = dpt_module.DPTDepthModel&#10;                print(&quot;DEBUG: Third import successful&quot;)&#10;            else:&#10;                raise ImportError(f&quot;Could not find midas module at {midas_path}&quot;)&#10;        except Exception as e3:&#10;            print(f&quot;DEBUG: All import attempts failed. Errors: {e1}, {e2}, {e3}&quot;)&#10;            raise ImportError(f&quot;Could not import DPTDepthModel. Tried multiple approaches. Last error: {e3}&quot;)&#10;&#10;import torchvision.transforms as transforms&#10;from app.models.object_detect import DetectClosestResponse, Detection&#10;&#10;router = APIRouter()&#10;&#10;# Load YOLO once (lazy load at module import)&#10;yolo_model = YOLO(&quot;models/yolo/yolov8l.pt&quot;)&#10;midas = DPTDepthModel(&#10;    path=&quot;models/midas/dpt_hybrid_384.pt&quot;,&#10;    backbone=&quot;vitb_rn50_384&quot;,&#10;    non_negative=True,&#10;)&#10;midas_transform = transforms.Compose([&#10;    transforms.Resize(384),&#10;    transforms.CenterCrop(384),&#10;    transforms.ToTensor(),&#10;    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),&#10;])&#10;device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)&#10;midas.to(device)&#10;midas.eval()&#10;&#10;&#10;def estimate_depth(image: np.ndarray) -&gt; np.ndarray:&#10;    &quot;&quot;&quot;Run MiDaS depth estimation and return normalized depth map.&quot;&quot;&quot;&#10;&#10;    # Convert OpenCV BGR -&gt; RGB and then to PIL&#10;    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)&#10;    pil_image = Image.fromarray(img_rgb)&#10;&#10;    # Apply MiDaS transform (returns a tensor, shape [3, H, W])&#10;    input_tensor = midas_transform(pil_image)&#10;&#10;    # Add batch dimension: [1, 3, H, W]&#10;    input_batch = input_tensor.unsqueeze(0).to(device)&#10;&#10;    with torch.no_grad():&#10;        prediction = midas(input_batch)&#10;        prediction = torch.nn.functional.interpolate(&#10;            prediction.unsqueeze(1),&#10;            size=image.shape[:2],&#10;            mode=&quot;bicubic&quot;,&#10;            align_corners=False,&#10;        ).squeeze()&#10;&#10;    depth_map = prediction.cpu().numpy()&#10;    # Normalize for easier comparison&#10;    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())&#10;&#10;    return depth_map&#10;&#10;@router.post(&quot;/detect_closest&quot;)&#10;async def detect_closest_objects(file: UploadFile = File(...), k: int = 3) -&gt; DetectClosestResponse:&#10;    try:&#10;        &quot;&quot;&quot;&#10;            Upload an image, detect objects with YOLO, estimate depth with MiDaS,&#10;            and return the k closest objects.&#10;            &quot;&quot;&quot;&#10;        # Read image into numpy array&#10;        &#10;        image_bytes = await file.read()&#10;        &#10;        nparr = np.frombuffer(image_bytes, np.uint8)&#10;        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)&#10;        &#10;        &#10;        # Step 1: Run YOLO&#10;        results = yolo_model(img)&#10;        &#10;        &#10;        # Step 2: Run depth estimation&#10;        depth_map = estimate_depth(img)&#10;        &#10;        &#10;        # Step 3: Collect detections with depth metrics&#10;        detections: List[Detection] = []&#10;        for r in results:&#10;            for box in r.boxes:&#10;                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())&#10;                label = r.names[int(box.cls)]&#10;                conf = float(box.conf)&#10;                &#10;                # Clip bounding box to image size&#10;                h, w = depth_map.shape&#10;                x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w - 1, x2), min(h - 1, y2)&#10;                &#10;                # Extract depth inside bounding box&#10;                roi = depth_map[y1:y2, x1:x2]&#10;                if roi.size == 0:&#10;                    continue&#10;                &#10;                # Compute closeness metrics&#10;                avg_depth = float(np.mean(roi))&#10;                min_depth = float(np.min(roi))  # closest pixel&#10;                median_depth = float(np.median(roi))&#10;                &#10;                detections.append(Detection(&#10;                        label=label,&#10;                        confidence=conf,&#10;                        bbox=[x1, y1, x2, y2],&#10;                        depth_avg=avg_depth,&#10;                        depth_min=min_depth,&#10;                        depth_median=median_depth,&#10;                    ))&#10;        filtered = [d for d in detections if d.label.lower() != &quot;person&quot;]&#10;        # Step 4: Sort by &quot;closeness&quot; (lowest depth = closest)&#10;        detections_sorted = sorted(filtered, key=lambda d: d.depth_min or 9999.0)&#10;        return DetectClosestResponse(closest_objects=detections_sorted[:k], all_objects=detections_sorted)&#10;    except Exception as e:&#10;        raise HTTPException(status_code=500, detail=str(e))" />
              <option name="updatedContent" value="# app/routers/object_router.py&#10;from typing import List&#10;import sys&#10;import os&#10;&#10;# Handle different directory structures between local and deployed environments&#10;current_dir = os.path.dirname(os.path.abspath(__file__))&#10;&#10;# Check if we're in a nested app/app structure (deployed) or app structure (local)&#10;if current_dir.endswith('/app/app/routers') or current_dir.endswith('\\app\\app\\routers'):&#10;    # Deployed environment: /app/app/routers -&gt; /app/models&#10;    project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))&#10;    models_dir = os.path.join(project_root, 'models')&#10;else:&#10;    # Local environment: project_root/app/routers -&gt; project_root/models&#10;    project_root = os.path.dirname(os.path.dirname(current_dir))&#10;    models_dir = os.path.join(project_root, 'models')&#10;&#10;# Add paths for import resolution&#10;sys.path.insert(0, models_dir)&#10;sys.path.insert(0, project_root)&#10;&#10;import torch&#10;from PIL import Image&#10;from fastapi import APIRouter, UploadFile, File, HTTPException&#10;from ultralytics import YOLO&#10;import cv2&#10;import numpy as np&#10;&#10;# Try multiple import approaches for maximum compatibility&#10;try:&#10;    print(&quot;DEBUG: Trying first import approach...&quot;)&#10;    from midas.dpt_depth import DPTDepthModel&#10;    print(&quot;DEBUG: First import successful&quot;)&#10;except ImportError as e1:&#10;    print(f&quot;DEBUG: First import failed: {e1}&quot;)&#10;    try:&#10;        print(&quot;DEBUG: Trying second import approach...&quot;)&#10;        from models.midas.dpt_depth import DPTDepthModel&#10;        print(&quot;DEBUG: Second import successful&quot;)&#10;    except ImportError as e2:&#10;        print(f&quot;DEBUG: Second import failed: {e2}&quot;)&#10;        try:&#10;            print(&quot;DEBUG: Trying third import approach (absolute path)...&quot;)&#10;            # Absolute path import as last resort&#10;            import importlib.util&#10;            midas_path = os.path.join(models_dir, 'midas', 'dpt_depth.py')&#10;            print(f&quot;DEBUG: Trying to load from: {midas_path}&quot;)&#10;            print(f&quot;DEBUG: File exists: {os.path.exists(midas_path)}&quot;)&#10;&#10;            if os.path.exists(midas_path):&#10;                spec = importlib.util.spec_from_file_location(&quot;dpt_depth&quot;, midas_path)&#10;                dpt_module = importlib.util.module_from_spec(spec)&#10;                spec.loader.exec_module(dpt_module)&#10;                DPTDepthModel = dpt_module.DPTDepthModel&#10;                print(&quot;DEBUG: Third import successful&quot;)&#10;            else:&#10;                raise ImportError(f&quot;Could not find midas module at {midas_path}&quot;)&#10;        except Exception as e3:&#10;            print(f&quot;DEBUG: All import attempts failed. Errors: {e1}, {e2}, {e3}&quot;)&#10;            raise ImportError(f&quot;Could not import DPTDepthModel. Tried multiple approaches. Last error: {e3}&quot;)&#10;&#10;import torchvision.transforms as transforms&#10;from app.models.object_detect import DetectClosestResponse, Detection&#10;&#10;router = APIRouter()&#10;&#10;# Load YOLO once (lazy load at module import)&#10;yolo_model = YOLO(&quot;models/yolo/yolov8l.pt&quot;)&#10;midas = DPTDepthModel(&#10;    path=&quot;models/midas/dpt_hybrid_384.pt&quot;,&#10;    backbone=&quot;vitb_rn50_384&quot;,&#10;    non_negative=True,&#10;)&#10;midas_transform = transforms.Compose([&#10;    transforms.Resize(384),&#10;    transforms.CenterCrop(384),&#10;    transforms.ToTensor(),&#10;    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),&#10;])&#10;device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)&#10;midas.to(device)&#10;midas.eval()&#10;&#10;&#10;def estimate_depth(image: np.ndarray) -&gt; np.ndarray:&#10;    &quot;&quot;&quot;Run MiDaS depth estimation and return normalized depth map.&quot;&quot;&quot;&#10;&#10;    # Convert OpenCV BGR -&gt; RGB and then to PIL&#10;    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)&#10;    pil_image = Image.fromarray(img_rgb)&#10;&#10;    # Apply MiDaS transform (returns a tensor, shape [3, H, W])&#10;    input_tensor = midas_transform(pil_image)&#10;&#10;    # Add batch dimension: [1, 3, H, W]&#10;    input_batch = input_tensor.unsqueeze(0).to(device)&#10;&#10;    with torch.no_grad():&#10;        prediction = midas(input_batch)&#10;        prediction = torch.nn.functional.interpolate(&#10;            prediction.unsqueeze(1),&#10;            size=image.shape[:2],&#10;            mode=&quot;bicubic&quot;,&#10;            align_corners=False,&#10;        ).squeeze()&#10;&#10;    depth_map = prediction.cpu().numpy()&#10;    # Normalize for easier comparison&#10;    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())&#10;&#10;    return depth_map&#10;&#10;@router.post(&quot;/detect_closest&quot;)&#10;async def detect_closest_objects(file: UploadFile = File(...), k: int = 3) -&gt; DetectClosestResponse:&#10;    try:&#10;        &quot;&quot;&quot;&#10;            Upload an image, detect objects with YOLO, estimate depth with MiDaS,&#10;            and return the k closest objects.&#10;            &quot;&quot;&quot;&#10;        # Read image into numpy array&#10;        &#10;        image_bytes = await file.read()&#10;        &#10;        nparr = np.frombuffer(image_bytes, np.uint8)&#10;        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)&#10;        &#10;        &#10;        # Step 1: Run YOLO&#10;        results = yolo_model(img)&#10;        &#10;        &#10;        # Step 2: Run depth estimation&#10;        depth_map = estimate_depth(img)&#10;        &#10;        &#10;        # Step 3: Collect detections with depth metrics&#10;        detections: List[Detection] = []&#10;        for r in results:&#10;            for box in r.boxes:&#10;                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())&#10;                label = r.names[int(box.cls)]&#10;                conf = float(box.conf)&#10;                &#10;                # Clip bounding box to image size&#10;                h, w = depth_map.shape&#10;                x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w - 1, x2), min(h - 1, y2)&#10;                &#10;                # Extract depth inside bounding box&#10;                roi = depth_map[y1:y2, x1:x2]&#10;                if roi.size == 0:&#10;                    continue&#10;                &#10;                # Compute closeness metrics&#10;                avg_depth = float(np.mean(roi))&#10;                min_depth = float(np.min(roi))  # closest pixel&#10;                median_depth = float(np.median(roi))&#10;                &#10;                detections.append(Detection(&#10;                        label=label,&#10;                        confidence=conf,&#10;                        bbox=[x1, y1, x2, y2],&#10;                        depth_avg=avg_depth,&#10;                        depth_min=min_depth,&#10;                        depth_median=median_depth,&#10;                    ))&#10;        filtered = [d for d in detections if d.label.lower() != &quot;person&quot;]&#10;        # Step 4: Sort by &quot;closeness&quot; (lowest depth = closest)&#10;        detections_sorted = sorted(filtered, key=lambda d: d.depth_min or 9999.0)&#10;        return DetectClosestResponse(closest_objects=detections_sorted[:k], all_objects=detections_sorted)&#10;    except Exception as e:&#10;        raise HTTPException(status_code=500, detail=str(e))" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>